{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1792eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "class Neural_Network:\n",
    "    # Initialize the network\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights, output_layer_weights, learning_rate):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "        self.hidden_layer_weights = hidden_layer_weights\n",
    "        self.output_layer_weights = output_layer_weights\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def sigmoid(self, inputs):\n",
    "        \n",
    "        output = 1/(1 + np.exp(-inputs))\n",
    "        return output\n",
    "\n",
    "    def forward_pass(self, inputs):\n",
    "        hidden_layer_outputs = []\n",
    "        hidden_biases = [-0.02,-0.2]\n",
    "        for i in range(self.num_hidden): #2\n",
    "            weighted_sum = 0.\n",
    "            for j in range(len(self.hidden_layer_weights)): #4\n",
    "                weighted_sum += inputs[j] * self.hidden_layer_weights[j][i]\n",
    "            output = self.sigmoid(weighted_sum)\n",
    "            hidden_layer_outputs.append(output)\n",
    "\n",
    "        output_layer_outputs = []\n",
    "        output_biases = [-0.33,0.26,0.06]\n",
    "        for i in range(self.num_outputs): #3\n",
    "            weighted_sum = 0.\n",
    "            for j in range(len(self.output_layer_weights)):#2\n",
    "                weighted_sum += hidden_layer_outputs[j] * self.output_layer_weights[j][i]\n",
    "            output = self.sigmoid(weighted_sum)\n",
    "            output_layer_outputs.append(output)\n",
    "        return hidden_layer_outputs, output_layer_outputs\n",
    "\n",
    "    def backward_propagate_error(self, inputs, hidden_layer_outputs, output_layer_outputs, desired_outputs):\n",
    "\n",
    "        output_layer_betas = np.zeros(self.num_outputs)\n",
    "        for i in range(self.num_outputs):\n",
    "            output_layer_betas[i] = desired_outputs[i] - output_layer_outputs[i]\n",
    "\n",
    "        hidden_layer_betas = np.zeros(self.num_hidden)\n",
    "        for i in range (self.num_hidden): \n",
    "            for j in range(self.num_outputs):\n",
    "                hidden_layer_betas[i] += self.output_layer_weights[i][j] * output_layer_outputs[j] * (1 - output_layer_outputs[j]) * output_layer_betas[j] \n",
    "\n",
    "        delta_output_layer_weights = np.zeros((self.num_hidden, self.num_outputs))\n",
    "        for i in range(self.num_hidden):\n",
    "            for j in range(self.num_outputs):\n",
    "                delta_output_layer_weights[i][j] = self.learning_rate * hidden_layer_outputs[i] * output_layer_outputs[j] * (1 - output_layer_outputs[j] ) * output_layer_betas[j] \n",
    "        \n",
    "        delta_hidden_layer_weights = np.zeros((self.num_inputs, self.num_hidden))\n",
    "        for i in range(self.num_inputs):\n",
    "            for j in range(self.num_hidden):\n",
    "                delta_hidden_layer_weights[i][j] = self.learning_rate * inputs[i] * hidden_layer_outputs[j] * ( 1 - hidden_layer_outputs[j]) * hidden_layer_betas[j] \n",
    "       \n",
    "        return delta_output_layer_weights, delta_hidden_layer_weights\n",
    "\n",
    "    def update_weights(self, delta_output_layer_weights, delta_hidden_layer_weights):\n",
    "        self.hidden_layer_weights += delta_hidden_layer_weights\n",
    "        self.output_layer_weights += delta_output_layer_weights\n",
    "\n",
    "    def train(self, instances, desired_outputs, epochs):\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            predictions = []\n",
    "            for i, instance in enumerate(instances):\n",
    "                hidden_layer_outputs, output_layer_outputs = self.forward_pass(instance)\n",
    "                delta_output_layer_weights, delta_hidden_layer_weights, = self.backward_propagate_error(\n",
    "                    instance, hidden_layer_outputs, output_layer_outputs, desired_outputs[i])\n",
    "                predicted_class = np.argmax(output_layer_outputs)   # TODO!\n",
    "                predictions.append(predicted_class)\n",
    "\n",
    "                self.update_weights(delta_output_layer_weights, delta_hidden_layer_weights)\n",
    "\n",
    "    def predict(self, instances):\n",
    "        predictions = []\n",
    "        for instance in instances:\n",
    "            hidden_layer_outputs, output_layer_outputs = self.forward_pass(instance)\n",
    "            predicted_class = np.argmax(output_layer_outputs)  # TODO! Should be 0, 1, or 2.\n",
    "            predictions.append(predicted_class)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "376bf622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    # encode 'Adelie' as 1, 'Chinstrap' as 2, 'Gentoo' as 3\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(labels)\n",
    "    # don't worry about this\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "\n",
    "    # encode 1 as [1, 0, 0], 2 as [0, 1, 0], and 3 as [0, 0, 1] (to fit with our network outputs!)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "    return label_encoder, integer_encoded, onehot_encoder, onehot_encoded\n",
    "\n",
    "\n",
    "def test_learning_rate(lr):\n",
    "    data = pd.read_csv('penguins307-train.csv')\n",
    "    labels = data.iloc[:, -1]\n",
    "    instances = data.iloc[:, :-1]\n",
    "    scaler = MinMaxScaler()\n",
    "    instances = scaler.fit_transform(instances)\n",
    "    label_encoder, integer_encoded, onehot_encoder, onehot_encoded = encode_labels(labels)\n",
    "\n",
    "    n_in = 4\n",
    "    n_hidden = 2\n",
    "    n_out = 3\n",
    "    learning_rate = lr\n",
    "\n",
    "    initial_hidden_layer_weights = np.array([[-0.28, -0.22], [0.08, 0.20], [-0.30, 0.32], [0.10, 0.01]])\n",
    "    initial_output_layer_weights = np.array([[-0.29, 0.03, 0.21], [0.08, 0.13, -0.36]])\n",
    "\n",
    "    nn = Neural_Network(n_in, n_hidden, n_out, initial_hidden_layer_weights, initial_output_layer_weights,\n",
    "                        learning_rate)\n",
    "\n",
    "\n",
    "    nn.train([instances[0]], [onehot_encoded[0]], 1)\n",
    "\n",
    "    nn.train(instances,onehot_encoded , 100)\n",
    "\n",
    "    pd_data_ts = pd.read_csv('penguins307-test.csv')\n",
    "    test_labels = pd_data_ts.iloc[:, -1]\n",
    "    test_instances = pd_data_ts.iloc[:, :-1]\n",
    "    #scale the test according to our training data.\n",
    "    test_instances = scaler.transform(test_instances)\n",
    "    pred = nn.predict(test_instances)\n",
    "    test_label_encoder, test_integer_encoded, test_onehot_encoder, test_onehot_encoded = encode_labels(test_labels)\n",
    "    test_labels = test_onehot_encoded\n",
    "    acc = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == np.argmax(test_labels[i]):\n",
    "            acc+=1\n",
    "    print('Accuracy in test set: = ', acc/len(test_labels))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18e5a980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test set: =  0.6461538461538462\n",
      "Accuracy in test set: =  0.8\n",
      "Accuracy in test set: =  0.4461538461538462\n",
      "Accuracy in test set: =  0.2\n",
      "Accuracy in test set: =  0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_learning_rate(10)\n",
    "test_learning_rate(0.2)\n",
    "test_learning_rate(0.001)\n",
    "test_learning_rate(0.0001)\n",
    "test_learning_rate(0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58317727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epochs(epoch):\n",
    "    data = pd.read_csv('penguins307-train.csv')\n",
    "    labels = data.iloc[:, -1]\n",
    "    instances = data.iloc[:, :-1]\n",
    "    scaler = MinMaxScaler()\n",
    "    instances = scaler.fit_transform(instances)\n",
    "    label_encoder, integer_encoded, onehot_encoder, onehot_encoded = encode_labels(labels)\n",
    "\n",
    "    n_in = 4\n",
    "    n_hidden = 2\n",
    "    n_out = 3\n",
    "    learning_rate = 0.2\n",
    "\n",
    "    initial_hidden_layer_weights = np.array([[-0.28, -0.22], [0.08, 0.20], [-0.30, 0.32], [0.10, 0.01]])\n",
    "    initial_output_layer_weights = np.array([[-0.29, 0.03, 0.21], [0.08, 0.13, -0.36]])\n",
    "\n",
    "    nn = Neural_Network(n_in, n_hidden, n_out, initial_hidden_layer_weights, initial_output_layer_weights,\n",
    "                        learning_rate)\n",
    "\n",
    "\n",
    "    nn.train([instances[0]], [onehot_encoded[0]], 1)\n",
    "\n",
    "    nn.train(instances,onehot_encoded , epoch)\n",
    "\n",
    "    pd_data_ts = pd.read_csv('penguins307-test.csv')\n",
    "    test_labels = pd_data_ts.iloc[:, -1]\n",
    "    test_instances = pd_data_ts.iloc[:, :-1]\n",
    "    #scale the test according to our training data.\n",
    "    test_instances = scaler.transform(test_instances)\n",
    "    pred = nn.predict(test_instances)\n",
    "    test_label_encoder, test_integer_encoded, test_onehot_encoder, test_onehot_encoded = encode_labels(test_labels)\n",
    "    test_labels = test_onehot_encoded\n",
    "    acc = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == np.argmax(test_labels[i]):\n",
    "            acc+=1\n",
    "    print('Accuracy in test set: = ', acc/len(test_labels))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25439044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test set: =  0.35384615384615387\n",
      "Accuracy in test set: =  0.8\n",
      "Accuracy in test set: =  0.8153846153846154\n",
      "Accuracy in test set: =  0.8923076923076924\n",
      "Accuracy in test set: =  0.9076923076923077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_epochs(1)\n",
    "test_epochs(10)\n",
    "test_epochs(100)\n",
    "test_epochs(500)\n",
    "test_epochs(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ea1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
